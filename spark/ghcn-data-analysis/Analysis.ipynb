{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91dc35fb-362a-4a3e-8b28-d9a2ab185472",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter notebook or Jupyter lab session running on the cluster master node in the cloud.\n",
    "\n",
    "Follow the instructions on the computing resources page to start a cluster and open this notebook.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Connect to the Windows server using Windows App.\n",
    "2. Connect to Kubernetes.\n",
    "3. Start Jupyter and open this notebook from Jupyter in order to connect to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19e7a858-eb85-42f0-ada8-91949347bb15",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "global azure_account_name\n",
    "global azure_data_container_name\n",
    "global azure_user_container_name\n",
    "global azure_user_token\n",
    "\n",
    "azure_account_name = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "azure_user_token = r\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(f\"fs.azure.sas.{azure_user_container_name}.{azure_account_name}.blob.core.windows.net\",  azure_user_token)\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44ac5093-826e-41c6-8843-cc3b4c76fff3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>twa78 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4043\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:00:18Z&se=2025-09-19T16:00:18Z&spr=https&sv=2022-11-02&sr=c&sig=qtg6fCdoFz6k3EJLw7dA8D3D8wN0neAYw8yG4z4Lw2o%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1743799780092</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/twa78/spark/</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2024-09-19T08:03:31Z&se=2025-09-19T16:03:31Z&spr=https&sv=2022-11-02&sr=c&sig=kMP%2BsBsRzdVVR8rrg%2BNbDhkRBNs6Q98kYY695XMRFDU%3D</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-7293fcadb56b4a51b9600f0f86ebe64a</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.sql.warehouse.dir</td><td>file:/home/twa78/spark-warehouse</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>twa78 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1743799780191</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>twa78-notebook-b48ba19602910660</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>twa78</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7077</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>1</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.02</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>twa78 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "#start_spark(executor_instances=4, executor_cores=2, worker_memory=4, master_memory=4)\n",
    "start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4979c30a-fba1-42b0-8cc0-a3134d3b808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here or insert cells below\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4026a93a-d171-418f-bd34-278f2e576e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wasbs://campus-data@madsstorage002.blob.core.windows.net/ghcnd/daily/*.csv.gz\n"
     ]
    }
   ],
   "source": [
    "# Define the input path for daily\n",
    "\n",
    "daily_relative_path = f'ghcnd/daily/*.csv.gz'\n",
    "daily_path = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/{daily_relative_path}'\n",
    "\n",
    "print(daily_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2a88770-a1c8-49e0-9031-701bb926dc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-05 10:22:22,619 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-04-05 10:22:22,879 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2025-04-05 10:22:22,926 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2025-04-05 10:22:22,926 INFO impl.MetricsSystemImpl: azure-file-system metrics system started\n",
      "Found 4 items\n",
      "-rw-r--r--   1 twa78 supergroup          0 2025-03-23 12:15 wasbs://campus-user@madsstorage002.blob.core.windows.net/twa78/stations/_SUCCESS\n",
      "-rw-r--r--   1 twa78 supergroup    1595800 2025-03-23 12:15 wasbs://campus-user@madsstorage002.blob.core.windows.net/twa78/stations/part-00000-306a0bde-153c-4181-a159-46062114d309-c000.snappy.parquet\n",
      "-rw-r--r--   1 twa78 supergroup    1608860 2025-03-23 12:15 wasbs://campus-user@madsstorage002.blob.core.windows.net/twa78/stations/part-00001-306a0bde-153c-4181-a159-46062114d309-c000.snappy.parquet\n",
      "-rw-r--r--   1 twa78 supergroup    1113540 2025-03-23 12:15 wasbs://campus-user@madsstorage002.blob.core.windows.net/twa78/stations/part-00002-306a0bde-153c-4181-a159-46062114d309-c000.snappy.parquet\n",
      "2025-04-05 10:22:23,318 INFO impl.MetricsSystemImpl: Stopping azure-file-system metrics system...\n",
      "2025-04-05 10:22:23,319 INFO impl.MetricsSystemImpl: azure-file-system metrics system stopped.\n",
      "2025-04-05 10:22:23,319 INFO impl.MetricsSystemImpl: azure-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "# Use the hdfs command to explore the data in Azure Blob Storage\n",
    "\n",
    "!hdfs dfs -ls wasbs://{azure_user_container_name}@{azure_account_name}.blob.core.windows.net/{username}/stations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4546dd0-5f2e-4a67-bb5e-569b2c303914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wasbs://campus-user@madsstorage002.blob.core.windows.net/twa78/stations\n"
     ]
    }
   ],
   "source": [
    "# Define the input path for stations dataframe defined in Processing\n",
    "stations_relative_path = f'{username}/stations'\n",
    "stations_path = f'wasbs://{azure_user_container_name}@{azure_account_name}.blob.core.windows.net/{stations_relative_path}'\n",
    "\n",
    "print(stations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73be54d2-ef84-4624-acf5-c513be017e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- STATE_ID: string (nullable = true)\n",
      " |-- COUNTRY_ID: string (nullable = true)\n",
      " |-- LATITUDE: string (nullable = true)\n",
      " |-- LONGITUDE: string (nullable = true)\n",
      " |-- ELEVATION: string (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- NETWORK: string (nullable = true)\n",
      " |-- CODE: string (nullable = true)\n",
      " |-- COUNTRY: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- first_year: string (nullable = true)\n",
      " |-- last_year: string (nullable = true)\n",
      " |-- distinct_elements_count: long (nullable = true)\n",
      " |-- core_elements_count: long (nullable = true)\n",
      " |-- other_elements_count: long (nullable = true)\n",
      "\n",
      "DataFrame[ID: string, STATE_ID: string, COUNTRY_ID: string, LATITUDE: string, LONGITUDE: string, ELEVATION: string, NAME: string, NETWORK: string, CODE: string, COUNTRY: string, STATE: string, first_year: string, last_year: string, distinct_elements_count: bigint, core_elements_count: bigint, other_elements_count: bigint]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+--------+---------+---------+------------------------+-------+-----+--------------------+-----+----------+---------+-----------------------+-------------------+--------------------+\n",
      "|ID         |STATE_ID|COUNTRY_ID|LATITUDE|LONGITUDE|ELEVATION|NAME                    |NETWORK|CODE |COUNTRY             |STATE|first_year|last_year|distinct_elements_count|core_elements_count|other_elements_count|\n",
      "+-----------+--------+----------+--------+---------+---------+------------------------+-------+-----+--------------------+-----+----------+---------+-----------------------+-------------------+--------------------+\n",
      "|ACW00011647|        |AC        |17.1333 |-61.7833 |19.      |ST JOHNS                |       |     |Antigua and Barbuda |NULL |1957      |1970     |7                      |5                  |2                   |\n",
      "|AE000041196|        |AE        |25.3330 |55.5170  |34.      |SHARJAH INTER. AIRP     |GSN    |41196|United Arab Emirates|NULL |1944      |2025     |4                      |3                  |1                   |\n",
      "|AEM00041194|        |AE        |25.2550 |55.3640  |10.      |DUBAI INTL              |       |41194|United Arab Emirates|NULL |1983      |2025     |4                      |3                  |1                   |\n",
      "|AEM00041218|        |AE        |24.2620 |55.6090  |264.     |AL AIN INTL             |       |41218|United Arab Emirates|NULL |1994      |2025     |4                      |3                  |1                   |\n",
      "|AG000060590|        |AG        |30.5667 |2.8667   |397.     |EL-GOLEA                |GSN    |60590|Algeria             |NULL |1892      |2025     |4                      |3                  |1                   |\n",
      "|AGE00147705|        |AG        |36.7800 |3.0700   |59.      |ALGIERS-VILLE/UNIVERSITE|       |     |Algeria             |NULL |1877      |1938     |3                      |3                  |0                   |\n",
      "|AGE00147706|        |AG        |36.8000 |3.0300   |344.     |ALGIERS-BOUZAREAH       |       |     |Algeria             |NULL |1893      |1920     |3                      |3                  |0                   |\n",
      "|AGE00147709|        |AG        |36.6300 |4.2000   |942.     |FORT NATIONAL           |       |     |Algeria             |NULL |1879      |1938     |3                      |3                  |0                   |\n",
      "|AGE00147711|        |AG        |36.3697 |6.6200   |660.     |CONSTANTINE             |       |     |Algeria             |NULL |1880      |1938     |3                      |3                  |0                   |\n",
      "|AGE00147715|        |AG        |35.4200 |8.1197   |863.     |TEBESSA                 |       |     |Algeria             |NULL |1879      |1938     |3                      |3                  |0                   |\n",
      "|AGE00147794|        |AG        |36.7800 |5.1000   |225.     |BEJAIA-CAP CARBON       |       |     |Algeria             |NULL |1926      |1938     |2                      |2                  |0                   |\n",
      "|AGM00060351|        |AG        |36.7950 |5.8740   |11.      |JIJEL                   |       |60351|Algeria             |NULL |1981      |2025     |4                      |3                  |1                   |\n",
      "|AGM00060353|        |AG        |36.8170 |5.8830   |6.       |JIJEL-PORT              |       |60353|Algeria             |NULL |1996      |2019     |4                      |3                  |1                   |\n",
      "|AGM00060369|        |AG        |36.7670 |3.1000   |12.      |ALGER-PORT              |       |60369|Algeria             |NULL |1943      |2025     |4                      |3                  |1                   |\n",
      "|AGM00060387|        |AG        |36.9170 |3.9500   |8.       |DELLYS                  |       |60387|Algeria             |NULL |1995      |2025     |4                      |3                  |1                   |\n",
      "|AGM00060402|        |AG        |36.7120 |5.0700   |6.       |SOUMMAM                 |       |60402|Algeria             |NULL |1973      |2025     |5                      |4                  |1                   |\n",
      "|AGM00060403|        |AG        |36.4670 |7.4670   |228.     |GUELMA                  |       |60403|Algeria             |NULL |1995      |2025     |5                      |4                  |1                   |\n",
      "|AGM00060430|        |AG        |36.3000 |2.2330   |721.     |MILIANA                 |       |60430|Algeria             |NULL |1957      |2025     |5                      |4                  |1                   |\n",
      "|AGM00060457|        |AG        |35.8830 |0.1170   |138.     |MOSTAGANEM              |       |60457|Algeria             |NULL |1976      |2025     |4                      |3                  |1                   |\n",
      "|AGM00060461|        |AG        |35.7000 |-0.6500  |22.      |ORAN-PORT               |       |60461|Algeria             |NULL |1995      |2025     |4                      |3                  |1                   |\n",
      "+-----------+--------+----------+--------+---------+---------+------------------------+-------+-----+--------------------+-----+----------+---------+-----------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the stations metadata into Spark from Azure Blob Storage using spark.read.text\n",
    "stations = spark.read.parquet(stations_path)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "print(type(stations))\n",
    "stations.printSchema()\n",
    "print(stations)\n",
    "stations.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a452a17e-8101-4bc5-a3ca-d4a3701b26b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129657 stations in stations.\n"
     ]
    }
   ],
   "source": [
    "# Total number of stations\n",
    "print(f'{stations.count()} stations in stations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18a6a41f-d052-4393-a598-b4e33f50bfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=============================>                             (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of active stations in 2025: 30735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Filter stations active in 2025\n",
    "active_2025 = stations.filter(F.col('last_year') >= 2025)\n",
    "active_2025_count = active_2025.count()\n",
    "print(f\"Number of active stations in 2025: {active_2025_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "939d679c-4a15-4918-9885-f7df0126d63e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|NETWORK| count|\n",
      "+-------+------+\n",
      "|       |127229|\n",
      "|    HCN|  1203|\n",
      "|    GSN|   976|\n",
      "|GSN HCN|    15|\n",
      "|    CRN|   234|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by network and count the number of stations for each network\n",
    "network_station_count = stations.groupBy('NETWORK').count()\n",
    "\n",
    "# Show the results\n",
    "network_station_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "018c83d6-6578-4fec-829e-0afaf165dada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stations in the Southern Hemisphere: 25316\n"
     ]
    }
   ],
   "source": [
    "# Count how many stations are in the southern hemisphere (lat<0)\n",
    "\n",
    "southern_stations = stations.filter(F.col('LATITUDE') < 0)\n",
    "southern_stations_count = southern_stations.count()\n",
    "print(f\"Number of stations in the Southern Hemisphere: {southern_stations_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47a724bc-9c93-4e17-a38a-d1487abe441a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stations in US territories: 414\n"
     ]
    }
   ],
   "source": [
    "# Count how many stations have country containing United States but not equal to.\n",
    "\n",
    "territory_stations = stations.filter(\n",
    "    (F.col(\"Country\") != \"United States\") & (F.col(\"Country\").contains('United States'))\n",
    ")\n",
    "territory_stations_count = territory_stations.count()\n",
    "print(f\"Number of stations in US territories: {territory_stations_count}\")\n",
    "\n",
    "#territory_stations.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "547b84ca-a128-468d-a0ac-e54545cdcdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/23 11:34:23 WARN AzureFileSystemThreadPoolExecutor: Disabling threads for Delete operation as thread count 0 is <= 1\n",
      "25/03/23 11:34:25 WARN AzureFileSystemThreadPoolExecutor: Disabling threads for Delete operation as thread count 0 is <= 1\n"
     ]
    }
   ],
   "source": [
    "# Count the number of stations by country, join with 'countries' and save to storage\n",
    "\n",
    "# Count\n",
    "country_station_counts = stations.groupBy(\"country\").agg(\n",
    "    F.count(\"*\").alias(\"total_stations_in_country\")\n",
    ")\n",
    "\n",
    "# Set countries input path\n",
    "countries_relative_path = f'ghcnd/ghcnd-countries.txt'\n",
    "countries_path = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/{countries_relative_path}'\n",
    "\n",
    "# Load the countries metadata into Spark from Azure Blob Storage using spark.read.text\n",
    "countries = spark.read.text(countries_path)\n",
    "\n",
    "# Use substring to extract parts of the string into new columns\n",
    "countries = countries.withColumn(\"COUNTRY_ID\", F.trim(F.substring(\"value\", 1, 2))) \\\n",
    "                     .withColumn(\"COUNTRY\", F.trim(F.substring(\"value\", 4, 100)))\n",
    "countries = countries.drop('value')\n",
    "\n",
    "# Join\n",
    "countries_enriched = countries.join(country_station_counts, on=\"country\", how=\"left\")\n",
    "#countries_enriched.show(20, False)\n",
    "\n",
    "# Define output path\n",
    "output_relative_path = f'{username}/countries'\n",
    "output_path = f'wasbs://{azure_user_container_name}@{azure_account_name}.blob.core.windows.net/{output_relative_path}'\n",
    "\n",
    "# Save to output\n",
    "countries_enriched.write.parquet(output_path, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "699ac920-58a3-48b3-ab51-1cc263bea4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/23 11:35:08 WARN AzureFileSystemThreadPoolExecutor: Disabling threads for Delete operation as thread count 0 is <= 1\n",
      "25/03/23 11:35:09 WARN AzureFileSystemThreadPoolExecutor: Disabling threads for Delete operation as thread count 0 is <= 1\n"
     ]
    }
   ],
   "source": [
    "# Count the number of stations by state (excluding NULL states), join with 'states' and save to storage\n",
    "\n",
    "# Count\n",
    "state_station_counts = stations.groupBy(\"state\").agg(\n",
    "    F.count(\"*\").alias(\"total_stations_in_state\")\n",
    ")\n",
    "\n",
    "# Set states input path\n",
    "states_relative_path = f'ghcnd/ghcnd-states.txt'\n",
    "states_path = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/{states_relative_path}'\n",
    "\n",
    "# Load the states metadata into Spark from Azure Blob Storage using spark.read.text\n",
    "states = spark.read.text(states_path)\n",
    "\n",
    "# Use substring to extract parts of the string into new columns\n",
    "states = states.withColumn(\"STATE_ID\", F.trim(F.substring(\"value\", 1, 2))) \\\n",
    "                     .withColumn(\"STATE\", F.trim(F.substring(\"value\", 4, 100)))\n",
    "states = states.drop('value')\n",
    "\n",
    "# Join\n",
    "states_enriched = states.join(state_station_counts, on=\"state\", how=\"left\")\n",
    "#states_enriched.show(20, False)\n",
    "\n",
    "# Define output path\n",
    "output_relative_path = f'{username}/states'\n",
    "output_path = f'wasbs://{azure_user_container_name}@{azure_account_name}.blob.core.windows.net/{output_relative_path}'\n",
    "\n",
    "# Save to output\n",
    "states_enriched.write.parquet(output_path, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1072b78e-96e0-4939-8d41-5ca418a199cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-23 11:35:00,578 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-03-23 11:35:00,842 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2025-03-23 11:35:00,891 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2025-03-23 11:35:00,891 INFO impl.MetricsSystemImpl: azure-file-system metrics system started\n",
      "Found 2 items\n",
      "-rw-r--r--   1 twa78 supergroup          0 2025-03-23 11:34 wasbs://campus-user@madsstorage002.blob.core.windows.net/twa78/countries/_SUCCESS\n",
      "-rw-r--r--   1 twa78 supergroup       4954 2025-03-23 11:34 wasbs://campus-user@madsstorage002.blob.core.windows.net/twa78/countries/part-00000-a009368e-c2b5-4db3-95b7-be9543b87879-c000.snappy.parquet\n",
      "2025-03-23 11:35:01,269 INFO impl.MetricsSystemImpl: Stopping azure-file-system metrics system...\n",
      "2025-03-23 11:35:01,269 INFO impl.MetricsSystemImpl: azure-file-system metrics system stopped.\n",
      "2025-03-23 11:35:01,269 INFO impl.MetricsSystemImpl: azure-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "# Use the hdfs command to explore the data in Azure Blob Storage\n",
    "\n",
    "!hdfs dfs -ls wasbs://{azure_user_container_name}@{azure_account_name}.blob.core.windows.net/{username}/countries/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d144c05d-c347-4015-9db4-c74212ac03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Define the Haversine function\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "    \n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lon1_rad = math.radians(lon1)\n",
    "    lat2_rad = math.radians(lat2)\n",
    "    lon2_rad = math.radians(lon2)\n",
    "    \n",
    "    # Differences in coordinates\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    \n",
    "    # Haversine formula\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    \n",
    "    # Distance in kilometers\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "# Define the UDF\n",
    "haversine_udf = F.udf(haversine, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52164dda-532e-4c88-9918-bc26156afa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+\n",
      "|         ID|         ID|distance_km|\n",
      "+-----------+-----------+-----------+\n",
      "|ACW00011647|ACW00011647|        0.0|\n",
      "|ACW00011647|AE000041196|  11749.993|\n",
      "|ACW00011647|AEM00041194|  11740.502|\n",
      "|ACW00011647|AEM00041218|  11814.122|\n",
      "|ACW00011647|AG000060590|  6656.2026|\n",
      "|AE000041196|ACW00011647|  11749.993|\n",
      "|AE000041196|AE000041196|        0.0|\n",
      "|AE000041196|AEM00041194|  17.658552|\n",
      "|AE000041196|AEM00041218|  119.45143|\n",
      "|AE000041196|AG000060590|   5158.444|\n",
      "|AEM00041194|ACW00011647|  11740.502|\n",
      "|AEM00041194|AE000041196|  17.658552|\n",
      "|AEM00041194|AEM00041194|        0.0|\n",
      "|AEM00041194|AEM00041218|  113.15391|\n",
      "|AEM00041194|AG000060590|   5146.737|\n",
      "|AEM00041218|ACW00011647|  11814.122|\n",
      "|AEM00041218|AE000041196|  119.45143|\n",
      "|AEM00041218|AEM00041194|  113.15391|\n",
      "|AEM00041218|AEM00041218|        0.0|\n",
      "|AEM00041218|AG000060590|   5206.985|\n",
      "+-----------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert 'latitude' and 'longitude' from strings to floats\n",
    "stations = stations.withColumn(\"latitude\", F.col(\"latitude\").cast(\"float\"))\n",
    "stations = stations.withColumn(\"longitude\", F.col(\"longitude\").cast(\"float\"))\n",
    "\n",
    "# Take a small subset of stations for testing\n",
    "stations_subset = stations.limit(5)\n",
    "\n",
    "# Perform a CROSS JOIN to get all pairs of stations\n",
    "station_pairs = stations_subset.alias(\"station1\").crossJoin(stations_subset.alias(\"station2\"))\n",
    "\n",
    "# Compute the distance for each pair of stations using the UDF\n",
    "distances = station_pairs.withColumn(\n",
    "    \"distance_km\", \n",
    "    haversine_udf(\n",
    "        station_pairs[\"station1.LATITUDE\"], station_pairs[\"station1.LONGITUDE\"],\n",
    "        station_pairs[\"station2.LATITUDE\"], station_pairs[\"station2.LONGITUDE\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "distances.select(\"station1.ID\", \"station2.ID\", \"distance_km\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebaa375f-56e2-4f5b-ada0-8eb89d6589b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/05 11:01:04 WARN ExtractPythonUDFFromJoinCondition: The join condition:(haversine(LATITUDE#150, LONGITUDE#167, LATITUDE#759, LONGITUDE#757)#792 > 0.0) of the join plan contains PythonUDF only, it will be moved out and the join plan will be turned to cross join.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+-------------------+-----------+-----------+\n",
      "|               NAME|         ID|               NAME|         ID|distance_km|\n",
      "+-------------------+-----------+-------------------+-----------+-----------+\n",
      "|WELLINGTON AERO AWS|NZM00093439|    PARAPARAUMU AWS|NZ000093417|   50.52885|\n",
      "|    PARAPARAUMU AWS|NZ000093417|WELLINGTON AERO AWS|NZM00093439|   50.52885|\n",
      "|WELLINGTON AERO AWS|NZM00093439|           KAIKOURA|NZM00093678|   151.0717|\n",
      "|           KAIKOURA|NZM00093678|WELLINGTON AERO AWS|NZM00093439|   151.0717|\n",
      "| HOKITIKA AERODROME|NZ000936150|  CHRISTCHURCH INTL|NZM00093781|  152.25804|\n",
      "|  CHRISTCHURCH INTL|NZM00093781| HOKITIKA AERODROME|NZ000936150|  152.25804|\n",
      "|  CHRISTCHURCH INTL|NZM00093781|           KAIKOURA|NZM00093678|  152.45882|\n",
      "|           KAIKOURA|NZM00093678|  CHRISTCHURCH INTL|NZM00093781|  152.45882|\n",
      "|    PARAPARAUMU AWS|NZ000093417|           KAIKOURA|NZM00093678|  199.52968|\n",
      "|           KAIKOURA|NZM00093678|    PARAPARAUMU AWS|NZ000093417|  199.52968|\n",
      "| HOKITIKA AERODROME|NZ000936150|         TARA HILLS|NZ000937470|  218.30919|\n",
      "|         TARA HILLS|NZ000937470| HOKITIKA AERODROME|NZ000936150|  218.30919|\n",
      "|   NEW PLYMOUTH AWS|NZ000933090|    PARAPARAUMU AWS|NZ000093417|  220.20021|\n",
      "|    PARAPARAUMU AWS|NZ000093417|   NEW PLYMOUTH AWS|NZ000933090|  220.20021|\n",
      "| HOKITIKA AERODROME|NZ000936150|           KAIKOURA|NZM00093678|  224.98088|\n",
      "|           KAIKOURA|NZM00093678| HOKITIKA AERODROME|NZ000936150|  224.98088|\n",
      "|   NEW PLYMOUTH AWS|NZ000933090|  AUCKLAND AERO AWS|NZM00093110|  230.70076|\n",
      "|  AUCKLAND AERO AWS|NZM00093110|   NEW PLYMOUTH AWS|NZ000933090|  230.70076|\n",
      "|  CHRISTCHURCH INTL|NZM00093781|         TARA HILLS|NZ000937470|   239.5304|\n",
      "|         TARA HILLS|NZ000937470|  CHRISTCHURCH INTL|NZM00093781|   239.5304|\n",
      "+-------------------+-----------+-------------------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nz_stations = stations.where(F.col('COUNTRY') == 'New Zealand')\n",
    "\n",
    "# Perform a CROSS JOIN to get all pairs of stations\n",
    "nz_station_pairs = nz_stations.alias(\"station1\").crossJoin(nz_stations.alias(\"station2\"))\n",
    "\n",
    "# Compute the distance for each pair of stations using the UDF\n",
    "nz_distances = nz_station_pairs.withColumn(\n",
    "    \"distance_km\", \n",
    "    haversine_udf(\n",
    "        nz_station_pairs[\"station1.LATITUDE\"], nz_station_pairs[\"station1.LONGITUDE\"],\n",
    "        nz_station_pairs[\"station2.LATITUDE\"], nz_station_pairs[\"station2.LONGITUDE\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "temp = nz_distances.select(\"station1.NAME\", \"station1.ID\", \"station2.NAME\", \"station2.ID\", \"distance_km\").orderBy('distance_km').filter(F.col('distance_km') > 0)\n",
    "nz_dist = temp.withColumnRenamed(\"station=2.NAME\", \"name2\")\n",
    "\n",
    "nz_dist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2c06ad2-687e-47b5-8b43-401a03463d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:======================================================>(105 + 1) / 106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of rows in 'daily is: 3139143397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Count the number of rows in 'daily'\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", StringType(), True),\n",
    "    StructField(\"DATE\", IntegerType(), True),\n",
    "    StructField(\"ELEMENT\", StringType(), True),\n",
    "    StructField(\"VALUE\", IntegerType(), True),\n",
    "    StructField(\"M-FLAG\", StringType(), True), \n",
    "    StructField(\"Q-FLAG\", StringType(), True), \n",
    "    StructField(\"S-FLAG\", StringType(), True),\n",
    "    StructField(\"OBS-TIME\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Load and count    \n",
    "daily = spark.read.csv(daily_path, schema=schema)\n",
    "row_count = daily.count()\n",
    "print(f\"The total number of rows in 'daily is: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15b814b1-833d-41a8-a7cb-3d64f0951541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+-----+------+------+------+--------+\n",
      "|ID         |DATE    |ELEMENT|VALUE|M-FLAG|Q-FLAG|S-FLAG|OBS-TIME|\n",
      "+-----------+--------+-------+-----+------+------+------+--------+\n",
      "|ASN00037091|20100101|PRCP   |0    |NULL  |NULL  |a     |NULL    |\n",
      "|ASN00037104|20100101|PRCP   |86   |NULL  |NULL  |a     |NULL    |\n",
      "|ASN00037107|20100101|PRCP   |0    |NULL  |NULL  |a     |NULL    |\n",
      "|ASN00037112|20100101|PRCP   |50   |NULL  |NULL  |a     |NULL    |\n",
      "|ASN00037119|20100101|PRCP   |0    |NULL  |NULL  |a     |NULL    |\n",
      "|ASN00037120|20100101|PRCP   |295  |NULL  |NULL  |a     |NULL    |\n",
      "|ASN00038000|20100101|TMAX   |404  |NULL  |NULL  |a     |NULL    |\n",
      "|ASN00038000|20100101|TMIN   |250  |NULL  |NULL  |a     |NULL    |\n",
      "|ASN00038000|20100101|PRCP   |4    |NULL  |NULL  |a     |NULL    |\n",
      "|ASN00038003|20100101|TMAX   |383  |NULL  |NULL  |a     |NULL    |\n",
      "|ASN00038003|20100101|TMIN   |236  |NULL  |NULL  |a     |NULL    |\n",
      "|ASN00038003|20100101|PRCP   |0    |NULL  |NULL  |a     |NULL    |\n",
      "|ASN00038004|20100101|PRCP   |92   |NULL  |NULL  |a     |NULL    |\n",
      "|ASN00038005|20100101|PRCP   |84   |NULL  |NULL  |a     |NULL    |\n",
      "|ASN00038009|20100101|PRCP   |0    |NULL  |NULL  |a     |NULL    |\n",
      "|ASN00038010|20100101|PRCP   |60   |NULL  |NULL  |a     |NULL    |\n",
      "|ASN00038012|20100101|PRCP   |0    |NULL  |NULL  |a     |NULL    |\n",
      "|ASN00038013|20100101|PRCP   |0    |NULL  |NULL  |a     |NULL    |\n",
      "|ASN00038014|20100101|PRCP   |0    |NULL  |NULL  |a     |NULL    |\n",
      "|ASN00038016|20100101|PRCP   |0    |NULL  |NULL  |a     |NULL    |\n",
      "+-----------+--------+-------+-----+------+------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daily.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23ed75e7-622e-4689-b25c-b60073ef4e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|ELEMENT|     count|\n",
      "+-------+----------+\n",
      "|   SNWD| 300711620|\n",
      "|   SNOW| 359249644|\n",
      "|   TMIN| 458928768|\n",
      "|   PRCP|1079767077|\n",
      "|   TMAX| 460114659|\n",
      "+-------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:=====================================================>(105 + 1) / 106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The element with the most observations is: PRCP with 1079767077 observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Obtain subset of 'daily' with only core elements, and calculate which is reported the most.\n",
    "\n",
    "# Define the core elements (from the inventory example)\n",
    "core_elements = [\"TMAX\", \"TMIN\", \"PRCP\", \"SNOW\", \"SNWD\"]\n",
    "\n",
    "# Filter the 'daily' dataset to include only rows with core elements\n",
    "daily_filtered = daily.filter(F.col(\"ELEMENT\").isin(core_elements))\n",
    "\n",
    "# Count the number of observations for each core element\n",
    "element_counts = daily_filtered.groupBy(\"ELEMENT\").count()\n",
    "\n",
    "# Show the counts for each core element\n",
    "element_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b894c394-fc29-4fbe-aa63-b2f7f9c7a8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:======================================================> (31 + 1) / 32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations of TMAX without a corresponding TMIN: 10660214\n",
      "Number of unique stations contributing to these observations: 28754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# How many observations of TMIN don't have a corresponding TMAX\n",
    "\n",
    "# Filter the 'daily' dataset for only TMAX and TMIN elements\n",
    "daily_filtered = daily.filter(F.col(\"ELEMENT\").isin([\"TMAX\", \"TMIN\"]))\n",
    "\n",
    "# Group by ID (station) and DATE (date), and collect the set of distinct elements reported\n",
    "grouped_data = daily_filtered.groupBy(\"ID\", \"DATE\").agg(\n",
    "    F.collect_set(\"ELEMENT\").alias(\"elements\")\n",
    ")\n",
    "\n",
    "# Filter rows where TMAX is reported but TMIN is missing\n",
    "missing_tmin = grouped_data.filter(~F.array_contains(grouped_data[\"elements\"], \"TMIN\")) \\\n",
    "    .filter(F.array_contains(grouped_data[\"elements\"], \"TMAX\"))\n",
    "\n",
    "# Count the number of observations where TMIN is missing for TMAX\n",
    "missing_tmin_count = missing_tmin.count()\n",
    "\n",
    "# Count the number of unique stations that contributed to these missing TMIN observations\n",
    "unique_stations = missing_tmin.select(\"ID\").distinct().count()\n",
    "\n",
    "# Output the results\n",
    "print(f\"Number of observations of TMAX without a corresponding TMIN: {missing_tmin_count}\")\n",
    "print(f\"Number of unique stations contributing to these observations: {unique_stations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e062eb2b-5866-43b6-8ceb-ff354d7136de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/05 11:04:16 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>twa78 (notebook)</code> is under the completed applications section in the Spark UI.</p><ul><li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    "\n",
    "stop_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c0865b-709e-4ce1-8278-f8362fe41070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
